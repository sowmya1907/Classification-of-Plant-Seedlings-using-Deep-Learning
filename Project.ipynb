{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/dheeraj/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Referred https://www.pyimagesearch.com/2017/NUM_CLASSES/11/image-classification-with-keras-and-deep-learning/\n",
    "# https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "# https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot\n",
    "\n",
    "# kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "from subprocess import check_output\n",
    "#list the files in the input directory\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "#print(check_output([\"pwd\",\"\"]).decode(\"utf8\")) # returns /kaggle/working\n",
    "#classes = check_output([\"ls\", \"../input/train\"]).decode(\"utf8\") # returns 12 directories\n",
    "#print((classes))\n",
    "def classes_to_int(label):\n",
    "    # label = classes.index(dir)\n",
    "    label = label.strip()\n",
    "    if label == \"Black-grass\":  return 0\n",
    "    if label == \"Charlock\":  return 1\n",
    "    if label == \"Cleavers\":  return 2\n",
    "    if label == \"Common Chickweed\":  return 3\n",
    "    if label == \"Common wheat\":  return 4\n",
    "    if label == \"Fat Hen\":  return 5\n",
    "    if label == \"Loose Silky-bent\": return 6\n",
    "    if label == \"Maize\":  return 7\n",
    "    if label == \"Scentless Mayweed\": return 8\n",
    "    if label == \"Shepherds Purse\": return 9\n",
    "    if label == \"Small-flowered Cranesbill\": return 10\n",
    "    if label == \"Sugar beet\": return 11\n",
    "    print(\"Invalid Label\", label)\n",
    "    return 12\n",
    "\n",
    "def int_to_classes(i):\n",
    "    if i == 0: return \"Black-grass\"\n",
    "    elif i == 1: return \"Charlock\"\n",
    "    elif i == 2: return \"Cleavers\"\n",
    "    elif i == 3: return \"Common Chickweed\"\n",
    "    elif i == 4: return \"Common wheat\"\n",
    "    elif i == 5: return \"Fat Hen\"\n",
    "    elif i == 6: return \"Loose Silky-bent\"\n",
    "    elif i == 7: return \"Maize\"\n",
    "    elif i == 8: return \"Scentless Mayweed\"\n",
    "    elif i == 9: return \"Shepherds Purse\"\n",
    "    elif i == 10: return \"Small-flowered Cranesbill\"\n",
    "    elif i == 11: return \"Sugar beet\"\n",
    "    print(\"Invalid class \", i)\n",
    "    return \"Invalid Class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Plant Seedlings Dataset contains images of approximately 960 unique plants belonging to\n",
    "# 12 species at several growth stages.\n",
    "# It comprises annotated RGB images with a physical resolution of roughly 10 pixels per mm.\n",
    "NUM_CLASSES = 12\n",
    "# we need images of same size so we convert them into the size\n",
    "WIDTH = 128\n",
    "HEIGHT = 128\n",
    "DEPTH = 3\n",
    "inputShape = (WIDTH, HEIGHT, DEPTH)\n",
    "# initialize number of epochs to train for, initial learning rate and batch size\n",
    "EPOCHS = 15\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "\n",
    "def readTrainData(trainDir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    # loop over the input images\n",
    "    dirs = os.listdir(trainDir) \n",
    "    for dir in dirs:\n",
    "        absDirPath = os.path.join(os.path.sep,trainDir, dir)\n",
    "        images = os.listdir(absDirPath)\n",
    "        for imageFileName in images:\n",
    "            # load the image, pre-process it, and store it in the data list\n",
    "            imageFullPath = os.path.join(trainDir, dir, imageFileName)\n",
    "            #print(imageFullPath)\n",
    "            img = load_img(imageFullPath)\n",
    "            arr = img_to_array(img)  # Numpy array with shape (233,233,3)\n",
    "            arr = cv2.resize(arr, (HEIGHT,WIDTH)) #Numpy array with shape (HEIGHT, WIDTH,3)\n",
    "            #print(arr.shape) \n",
    "            data.append(arr)\n",
    "            label = classes_to_int(dir)\n",
    "            labels.append(label)\n",
    "    return data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    model = Sequential()\n",
    "    # first set of CONV => RELU => POOL layers\n",
    "    # The CONV  layer will learn 20 convolution filters, each of which are 5×5.\n",
    "    model.add(Conv2D(20, (5, 5), padding=\"same\", input_shape=inputShape))\n",
    "    # We then apply a ReLU activation function followed by 2×2 max-pooling in both \n",
    "    # the x and y direction with a stride of two. \n",
    "    #To visualize this operation, consider a sliding window that “slides” across \n",
    "    #the activation volume, taking the max operation over each region, while taking \n",
    "    #a step of two pixels in both the horizontal and vertical direction.\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    # second set of CONV => RELU => POOL layers\n",
    "    #This time we are learning 50 convolutional filters rather than the 20 convolutional\n",
    "    #filters as in the previous layer set. It’s common to see the number of CONV \n",
    "    #filters learned increase the deeper we go in the network architecture.\n",
    "    model.add(Conv2D(50, (5, 5), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    # first (and only) set of FC => RELU layers\n",
    "    # Flattening out the volume into a set of fully-connected layers\n",
    "    # Take the output of the preceding MaxPooling2D layer and flatten it into a single vector.\n",
    "    # This operation allows us to apply our dense/fully-connected layers.\n",
    "    # Fully-connected layer contains 500 nodes which is passed through another \n",
    "    # nonlinear ReLU activation.\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    # softmax classifier\n",
    "    # Another fully-connected layer, but this one is special — the number of nodes is equal \n",
    "    # to the number of classes  (i.e., the classes we want to recognize).\n",
    "    # This Dense layer is then fed into our softmax classifier\n",
    "    # which will yield the probability for each class.\n",
    "    model.add(Dense(output_dim=12))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    # returns our fully constructed deep learning + Keras image classifier \n",
    "    opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "    # use binary_crossentropy if there are two classes\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n",
      "Partition data into 75:25...\n",
      "Generating images...\n",
      "compiling model...\n",
      "WARNING:tensorflow:From /home/dheeraj/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "training network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dheeraj/.local/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=12)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dheeraj/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/15\n",
      "111/111 [==============================] - 53s 473ms/step - loss: 2.4928 - accuracy: 0.1436 - val_loss: 2.3959 - val_accuracy: 0.2189\n",
      "Epoch 2/15\n",
      "111/111 [==============================] - 52s 471ms/step - loss: 2.1691 - accuracy: 0.2606 - val_loss: 2.1196 - val_accuracy: 0.2449\n",
      "Epoch 3/15\n",
      "111/111 [==============================] - 52s 471ms/step - loss: 1.5626 - accuracy: 0.4448 - val_loss: 1.3364 - val_accuracy: 0.5556\n",
      "Epoch 4/15\n",
      "111/111 [==============================] - 52s 470ms/step - loss: 1.3986 - accuracy: 0.5105 - val_loss: 1.1252 - val_accuracy: 0.5985\n",
      "Epoch 5/15\n",
      "111/111 [==============================] - 53s 473ms/step - loss: 1.3156 - accuracy: 0.5462 - val_loss: 1.1664 - val_accuracy: 0.6052\n",
      "Epoch 6/15\n",
      "111/111 [==============================] - 52s 470ms/step - loss: 1.2318 - accuracy: 0.5592 - val_loss: 1.0661 - val_accuracy: 0.6456\n",
      "Epoch 7/15\n",
      "111/111 [==============================] - 52s 468ms/step - loss: 1.2133 - accuracy: 0.5824 - val_loss: 1.0211 - val_accuracy: 0.6566\n",
      "Epoch 8/15\n",
      "111/111 [==============================] - 52s 469ms/step - loss: 1.1477 - accuracy: 0.6028 - val_loss: 1.0332 - val_accuracy: 0.6684\n",
      "Epoch 9/15\n",
      "111/111 [==============================] - 52s 466ms/step - loss: 1.0362 - accuracy: 0.6380 - val_loss: 0.8723 - val_accuracy: 0.7096\n",
      "Epoch 10/15\n",
      "111/111 [==============================] - 52s 468ms/step - loss: 1.0415 - accuracy: 0.6337 - val_loss: 0.9968 - val_accuracy: 0.6591\n",
      "Epoch 11/15\n",
      "111/111 [==============================] - 52s 469ms/step - loss: 1.0085 - accuracy: 0.6527 - val_loss: 0.8425 - val_accuracy: 0.7113\n",
      "Epoch 12/15\n",
      "111/111 [==============================] - 52s 468ms/step - loss: 0.9212 - accuracy: 0.6929 - val_loss: 0.7782 - val_accuracy: 0.7475\n",
      "Epoch 13/15\n",
      "111/111 [==============================] - 52s 468ms/step - loss: 0.8587 - accuracy: 0.7048 - val_loss: 0.7425 - val_accuracy: 0.7576\n",
      "Epoch 14/15\n",
      "111/111 [==============================] - 52s 469ms/step - loss: 0.8601 - accuracy: 0.6969 - val_loss: 0.7589 - val_accuracy: 0.7416\n",
      "Epoch 15/15\n",
      "111/111 [==============================] - 52s 468ms/step - loss: 0.8480 - accuracy: 0.7147 - val_loss: 0.7022 - val_accuracy: 0.7643\n",
      "Saving model to disk\n",
      "Model successfully trained\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "allLabels =  os.listdir(\"/home/dheeraj/Documents/train/\")  # list of subdirectories and files\n",
    "print(\"Loading images...\")\n",
    "sys.stdout.flush()\n",
    "X, Y = readTrainData(\"/home/dheeraj/Documents/train/\")\n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "X = np.array(X, dtype=\"float\") / 255.0\n",
    "Y = np.array(Y)\n",
    "# convert the labels from integers to vectors\n",
    "Y =  to_categorical(Y, num_classes=12)\n",
    "\n",
    "print(\"Partition data into 75:25...\")\n",
    "sys.stdout.flush()\n",
    "# partition the data into training and testing splits using 75% training and 25% for validation\n",
    "(trainX, valX, trainY, valY) = train_test_split(X,Y,test_size=0.25, random_state=10)\n",
    "\n",
    "#construct the image generator for data augmentation\n",
    "print(\"Generating images...\")\n",
    "sys.stdout.flush()\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, \\\n",
    "    height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\\\n",
    "    horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "# initialize the model\n",
    "print(\"compiling model...\")\n",
    "sys.stdout.flush()\n",
    "model = createModel()\n",
    "# train the network\n",
    "print(\"training network...\")\n",
    "sys.stdout.flush()\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS), \\\n",
    "    validation_data=(valX, valY), \\\n",
    "    steps_per_epoch=len(trainX) // BS, epochs=EPOCHS, verbose=1)\n",
    "\n",
    "# save the model to disk\n",
    "print(\"Saving model to disk\")\n",
    "sys.stdout.flush()\n",
    "model.save(\"/tmp/mymodel\")\n",
    "\n",
    "print('Model successfully trained')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You chose: /home/dheeraj/Documents/anothertest\n",
      "\n",
      "file_path_variable =  /home/dheeraj/Documents/anothertest\n",
      "3/3 [==============================] - 0s 116ms/step\n",
      "Writing complete\n"
     ]
    }
   ],
   "source": [
    "def readTestData(testDir):\n",
    "    data = []\n",
    "    filenames = []\n",
    "    # loop over the input images\n",
    "    images = os.listdir(testDir)\n",
    "    for imageFileName in images:\n",
    "        # load the image, pre-process it, and store it in the data list\n",
    "        imageFullPath = os.path.join(testDir, imageFileName)\n",
    "        #print(imageFullPath)\n",
    "        img = load_img(imageFullPath)\n",
    "        arr = img_to_array(img)  # Numpy array with shape (...,..,3)\n",
    "        arr = cv2.resize(arr, (HEIGHT,WIDTH)) \n",
    "        data.append(arr)\n",
    "        filenames.append(imageFileName)\n",
    "    return data, filenames\n",
    "\n",
    "# read test data and find its classification\n",
    "import tkinter\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "\n",
    "root = tkinter.Tk()\n",
    "root.withdraw() #use to hide tkinter window\n",
    "\n",
    "def search_for_file_path ():\n",
    "    currdir = os.getcwd()\n",
    "    tempdir = filedialog.askdirectory(parent=root, initialdir=currdir, title='Please select a directory')\n",
    "    if len(tempdir) > 0:\n",
    "        print (\"You chose: %s\" % tempdir)\n",
    "    return tempdir\n",
    "\n",
    "\n",
    "file_path_variable = search_for_file_path()\n",
    "print (\"\\nfile_path_variable = \", file_path_variable)\n",
    "\n",
    "\n",
    "testX, filenames = readTestData(str(file_path_variable))\n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "testX = np.array(testX, dtype=\"float\") / 255.0\n",
    "\n",
    "from keras.models import load_model\n",
    "mymodel = load_model('/tmp/mymodel')\n",
    "yFit = mymodel.predict(testX, batch_size=10, verbose=1)\n",
    "\n",
    "#print(type(yFit)) # numpy.ndarray\n",
    "#print(type(filenames)) # list\n",
    "\n",
    "import csv  \n",
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['file', 'species']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for index, file in enumerate(filenames):\n",
    "        classesProbs = yFit[index]\n",
    "        maxIdx = 0\n",
    "        maxProb = 0;\n",
    "        for idx in range(0,11):\n",
    "            if(classesProbs[idx] > maxProb):\n",
    "                maxIdx = idx\n",
    "                maxProb = classesProbs[idx]\n",
    "        writer.writerow({'file': file, 'species': int_to_classes(maxIdx)})\n",
    "print(\"Writing complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         file           species\n",
      "0                  Sugar Beet  Loose Silky-bent\n",
      "1             Shepherds Purse  Common Chickweed\n",
      "2                Common Wheat  Loose Silky-bent\n",
      "3   Small flowered Cranesbill  Common Chickweed\n",
      "4                       Maize  Common Chickweed\n",
      "5           Scentless Mayweed  Common Chickweed\n",
      "6            Loose Silky Bent  Common Chickweed\n",
      "7                    Cleavers  Loose Silky-bent\n",
      "8                     Fat Hen  Loose Silky-bent\n",
      "9                    Charlock  Common Chickweed\n",
      "10           Common Chickweed  Common Chickweed\n",
      "11                Black grass  Loose Silky-bent\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import pandas \n",
    "root=tk()\n",
    "df=pandas.read_csv('output.csv')\n",
    "w=Label(root, text=str(df))\n",
    "w.pack()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
